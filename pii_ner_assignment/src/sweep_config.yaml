# sweep_config.yaml
program: src/train.py # Path to the training script
method: bayes        # Bayesian optimization
name: pii-ner-tuning

# Configuration for the metric we want to track/optimize
# NOTE: We are tracking loss here, but you should manually check the PII Precision
# for the final choice using your eval_span_f1.py script.
metric:
  name: train_loss # Using train_loss as a quick proxy for sweep progress
  goal: minimize

parameters:
  # Fixed parameters
  model_name:
    value: distilbert-base-uncased
  train:
    value: data/train_unbiased.jsonl # <-- IMPORTANT: Use the unbiased, large file
  dev:
    value: data/dev_unbiased.jsonl   # <-- IMPORTANT: Use the unbiased, large file
  out_dir:
    value: out/sweep_run

  # Hyperparameters to sweep over
  epochs:
    value: 13 # Fixed at 10 epochs for fair comparison within the sweep
  batch_size:
    values: [8, 16, 32, 64] # Increased batch size for potential speedup/stability
  lr:
    min: 1e-5
    max: 5e-5        # Optimal learning rate range for BERT fine-tuning
    distribution: uniform
  max_length:
    values: [64, 128, 256] # Shorter sequence length reduces latency but may cut off long noisy utterances